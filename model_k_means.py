# -*- coding: utf-8 -*-
"""Model_K_Means.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Csq55Bz-6PomOp2e7lmlq4xDXnysTivl
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from math import radians, sin, cos, sqrt, atan2
from joblib import dump, load
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

from google.colab import files
uploaded = files.upload()

restaurant = pd.read_csv('restaurant.csv')
provinsi = pd.read_csv('provinsi.csv')
makanan_khas = pd.read_csv('makanan_khas.csv')
rating = pd.read_csv('reting.csv')

restaurant_provinsi = pd.merge(restaurant, provinsi, on='id_provinsi')
restaurant_provinsi_makanan_khas = pd.merge(restaurant_provinsi, makanan_khas, on='id_makanan')
data_gabungan = pd.merge(restaurant_provinsi_makanan_khas, rating, on='id_reting')
data_gabungan = data_gabungan[['id_restaurant','id_reting', 'nama_tempat', 'alamat', 'id_provinsi', 'nama_provinsi', 'id_makanan', 'harga', 'deskripsi', 'long', 'lat']]

class_column = 'id_provinsi'
plt.figure(figsize=(10, 6))
sns.countplot(data=data_gabungan, x=class_column, order=data_gabungan[class_column].value_counts().index)
plt.title('Distribusi Jumlah Sampel per Kelas')
plt.xlabel('Kelas')
plt.ylabel('Jumlah Sampel')
plt.xticks(rotation=90)
plt.show()

samples_per_class = data_gabungan[class_column].value_counts()
print(f"Jumlah sampel per kelas:\n{samples_per_class}")

samples_per_class = data_gabungan[class_column].value_counts()
print(f"Statistik Jumlah Sampel per Kelas:\n{samples_per_class.describe()}")
print(f"Jumlah sampel minimal dalam satu kelas: {samples_per_class.min()}")
print(f"Jumlah sampel maksimal dalam satu kelas: {samples_per_class.max()}")

data_gabungan.isnull().sum().sort_values(ascending = False)

data_gabungan.info()

data_gabungan.duplicated().sum()

def haversine(lat1, lon1, lat2, lon2):
    R = 6371.0
    dlat = radians(lat2 - lat1)
    dlon = radians(lon2 - lon1)
    a = sin(dlat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))
    distance = R * c
    return distance

user_lat = -6.200000
user_lon = 106.816666

data_gabungan['distance'] = data_gabungan.apply(lambda row: haversine(user_lat, user_lon, row['lat'], row['long']), axis=1)

# Filter restoran yang berjarak maksimal 20 km
filtered_restaurants = data_gabungan[data_gabungan['distance'] <= 20]

# Standardisasi fitur
scaler = StandardScaler()
scaled_features = scaler.fit_transform(filtered_restaurants[['lat', 'long']])

# K-Means Clustering
kmeans = KMeans(n_clusters=5, random_state=42)
filtered_restaurants['cluster'] = kmeans.fit_predict(scaled_features)

# Pisahkan data menjadi set pelatihan dan pengujian
X_train, X_test, y_train, y_test = train_test_split(
    scaled_features, filtered_restaurants['cluster'], test_size=0.2, random_state=42)

# Mengonversi label ke bentuk categorical
y_train_cat = to_categorical(y_train, num_classes=5)
y_test_cat = to_categorical(y_test, num_classes=5)

# Membangun model TensorFlow
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(2,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Melatih model
history = model.fit(X_train, y_train_cat, epochs=150, validation_split=0.2, verbose=2)

# Evaluasi model pada set pelatihan
train_loss, train_accuracy = model.evaluate(X_train, y_train_cat, verbose=2)
print(f'Akurasi set pelatihan: {train_accuracy:.2f}%, Loss set pelatihan: {train_loss:.2f}')

# Evaluasi model pada set pengujian
test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=2)
print('Akurasi model pada set pengujian: {:5.2f}%, Loss set pengujian: {:.2f}'.format(100 * test_accuracy, test_loss))

# Visualisasi kurva loss
epochs = range(1, len(history.history['loss']) + 1)
loss = history.history['loss']
val_loss = history.history['val_loss']

plt.plot(epochs, val_loss, '-', label='val_loss')
plt.plot(epochs, loss, 'r-', label='loss')

plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Tampilkan hasil restoran yang berada dalam jarak 20 km
print("Restoran yang berada dalam jarak 20 km dari pengguna:")
print(filtered_restaurants[['id_restaurant', 'id_reting', 'id_makanan', 'id_provinsi', 'nama_tempat', 'harga', 'alamat', 'deskripsi', 'long', 'lat', 'distance', 'cluster']].to_string(index=False))

# Menyimpan bobot model ke file HDF5
model.save("model.h5")

# Membuat converter dari model Keras yang telah dilatih
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# Mengonversi model ke format TFLite
tflite_model = converter.convert()

# Menyimpan model TFLite ke file
tflite_model_file = 'model.tflite'
with open(tflite_model_file, 'wb') as f:
    f.write(tflite_model)

print(f"Model TFLite telah disimpan ke file: {tflite_model_file}")